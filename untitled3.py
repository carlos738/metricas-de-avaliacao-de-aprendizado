# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BTuc2jauY0skNV8BD1oq6tSE2Y-Q9gri
"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# criando parâmetros para simulação de uma matriz de confusão
num_classes = 12
y_true = np.random.randint(num_classes, size=100)
y_pred = np.random.randint(num_classes, size=100)
accuracy_score = np.mean(y_true == y_pred)

# Gerando y_true e y_pred
y_true = np.random.randint(num_classes,size=100)
y_pred = np.random.randint(num_classes,size=100)

for true_label,predicted_label in zip(y_true,y_pred):
  print(f"True Label:{true_label} Predicted Label:{predicted_label}");

# predição correta
correct_predictions = np.sum(y_true == y_pred)

# Predição incorreta
incorret_predictions = np.sum(y_true != y_pred)

# gerar a matriz confusão
confusion_matrix = confusion_matrix(y_true,y_pred)

# plotar a matriz confusão
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix,annot=True,fmt="d",cmap="Blues",cbar=False)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()

# sensibilidade
sensitivity = np.diag(confusion_matrix) / np.sum(confusion_matrix,axis=1)

# Calcular a especificidade para cada classe
# Especificidade = TN / (TN + FP)
specificity = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=0)

# TN: "True Negatives" (Soma de todos os elementos, excluindo a linha e coluna da classe)
tn = np.sum(confusion_matrix) - np.diag(confusion_matrix) -np.sum(confusion_matrix,axis=0) - np.sum(confusion_matrix,axis=1)
print(tn)

# FP: "False Positives" (Soma da coluna da classe, exluindo o elemento da diagonal)
fp = np.sum(confusion_matrix,axis=0) - np.diag(confusion_matrix)
print(fp)

# calculo de acuracidade
accuracy = np.mean(np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=1))
print(accuracy)

# Função para calcular a precisão manual
def claculate_precision(confusion_matrix,class_label):
  col = confusion_matrix[:,class_label]
  return confusion_matrix[class_label,class_label] / col.sum()
  print(col)

# Calcula o F-score para cada classe
f_scores = []
for i in range(num_classes):
  precision = claculate_precision(confusion_matrix,1)
  recall = sensitivity[i]
  f1_score = 2 * (precision * recall) / (precision + recall)
  f_scores.append(f1_score)
  print(f"Class {i}: Precision={precision:.4f}, Recall={recall:.4f}, F1-Score={f1_score:.4f}")

# F-score formula
f_score = 2 *(precision * recall) / (precision + recall)
print(f_score)

# gerar o relatorio de clasificação
from sklearn.metrics import classification_report
classification_report = classification_report(y_true,y_pred)
print(classification_report)